{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1q/tJ0RTsRS5B6F4jWypX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elektromusik/RAG/blob/main/RAG_with_Metadata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG with Metadata."
      ],
      "metadata": {
        "id": "ljEqpEapKaUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Packages."
      ],
      "metadata": {
        "id": "-vuyBztpIzt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip --quiet install faiss-cpu langchain langchain_community langchain_mistralai\n",
        "!pip --quiet install pypdf sentence_transformers\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from os import getenv"
      ],
      "metadata": {
        "id": "diYSsATQZjhB"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing."
      ],
      "metadata": {
        "id": "cJtCZCx8I7FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all .pdf Documents in some Folder.\n",
        "loader = PyPDFDirectoryLoader(\"/content/\", glob=\"*.pdf\")\n",
        "pages = loader.load()\n",
        "\n",
        "# Choose the Chunk Size.\n",
        "# [1 page ~ 700 words. 1 chunk <= 256 words (due to the embedding model).\n",
        "# 1 word ~ 4.7 characters. So, 1 chunk <= 1000 characters, otherwise it is\n",
        "# truncated. Alltogether, we end up with at least 3 chunks per page at a\n",
        "# chunk_size of 1000.]\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                              chunk_size    =1000,\n",
        "                              chunk_overlap = 100,\n",
        "                              separators = [\"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"])\n",
        "\n",
        "chunks = [{\"page_content\" : text_splitter.split_text(pages[i].page_content),\n",
        "           \"metadata\" : pages[i].metadata} for i in range(len(pages))]\n",
        "\n",
        "# Create a List of Langchain Documents.\n",
        "# [They include content and metadata, i. e. 'source' and 'page'.]\n",
        "Documents = [Document(page_content=j, metadata=chunk['metadata']) for chunk in\n",
        "             chunks for j in chunk['page_content']]\n",
        "# Alternative:\n",
        "# Documents=[]\n",
        "# for i in range(len(pages)):\n",
        "#   for j in chunks[i]['page_content']:\n",
        "#     Documents.append(Document(page_content=j, metadata=chunks[i]['metadata']))\n",
        "\n",
        "print(Documents[5])\n",
        "\n",
        "# Choose the Embedding Model.\n",
        "# [I tried to find the best embedding model via the MTEB leaderboard at\n",
        "# huggingface.co:\n",
        "# 1) nvidia/NV-Embed-v2 (not found on NVIDIA website),\n",
        "# 2) BAAI/bge-en-icl (runs forever)],\n",
        "# ...\n",
        "# 10) nvidia/NV-Embed-v1 (needed packages incompatible).\n",
        "# Hence, I ended up with the following standard model. The problem with this\n",
        "# model is, that it truncates above 257 words ~ 1000 characters.]\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Add the Chunks and the Metadata to the Vector Database.\n",
        "vectorstore = FAISS.from_documents(documents=Documents,\n",
        "                                   embedding=embedding_model)"
      ],
      "metadata": {
        "id": "9z-4QXRkb5V3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a06813-55a0-4dca-8cdc-a416faab589c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='very grave, hesitant faces. Father agreed to finance me for a year and after various delays I came \n",
            "east, permanently, I thought, in the spring of twenty-two.\n",
            "The practical thing was to find rooms in the city but it was a warm season and I had just left a \n",
            "country of wide lawns and friendly trees, so when a young man at the office suggested that we take \n",
            "a house together in a commuting town it sounded like a great idea. He found the house, a weather \n",
            "beaten cardboard bungalow at eighty a month, but at the last minute the firm ordered him to \n",
            "Washington and I went out to the country alone. I had a dog, at least I had him for a few days until \n",
            "he ran away, and an old Dodge and a Finnish woman who made my bed and cooked breakfast and \n",
            "muttered Finnish wisdom to herself over the electric stove.\n",
            "It was lonely for a day or so until one morning some man, more recently arrived than I, stopped me \n",
            "on the road.\n",
            "\"How do you get to West Egg village?\" he asked helplessly.' metadata={'source': '/content/The_Great_Gatsby_Part_1.pdf', 'page': 1}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Components of RAG."
      ],
      "metadata": {
        "id": "TbmITKZoJI5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever.\n",
        "# [Similarity search with a threshold: search_type=\"similarity_score_threshold\",\n",
        "# search_kwargs={\"score_threshold\": 0.05}]\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Systemprompt.\n",
        "template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context:  {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Set LLM.\n",
        "llm = ChatMistralAI(mistral_api_key=\"QlvclnycvhnkP808e4HS0BWz0kwZU06j\")\n",
        "\n",
        "# Create Pipeline (Retrieve-Augment-Generate)\n",
        "RAG_chain = ({\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "              | prompt\n",
        "              | llm\n",
        "              | StrOutputParser())"
      ],
      "metadata": {
        "id": "cRZpR1VvJgzq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A."
      ],
      "metadata": {
        "id": "6-UOB5h4KLP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "KHJ0JxcdoI5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate\n",
        "query = \"\"\"Who was invited to the parties? On which pages are they listed?\"\"\"\n",
        "RAG_chain.invoke(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "tpM1u6TGGuoF",
        "outputId": "b443e432-d6ca-4f5d-8826-d86e99174970"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The individuals invited to Gatsby's parties include Nick Carraway, as mentioned on page 20 of the first document. Uninvited guests also attended, as stated on page 23 of the first document and page 6 of the second document. The list of attendees is not explicitly provided in the context.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rBs8FqlNDGAE"
      }
    }
  ]
}