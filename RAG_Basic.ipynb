{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elektromusik/RAG/blob/main/RAG_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrNP404M7r5"
      },
      "source": [
        "# RAG with Langchain and Mistral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgqxGDjYNBWv"
      },
      "source": [
        "## Load Packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WXApMi-sJd9v"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet faiss-cpu langchain langchain_community langchain_mistralai\n",
        "!pip install --quiet sentence_transformers\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWz4RVK5NE5c"
      },
      "source": [
        "## Data Preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3GWp5u52I5_2"
      },
      "outputs": [],
      "source": [
        "# Import the Text.\n",
        "with open(\"The_Great_Gatsby.txt\") as x:\n",
        "  text = x.read()\n",
        "\n",
        "# Chunk the Text\n",
        "# [1 page ~ 700 words. 1 chunk <= 256 words (due to the embedding model).\n",
        "# 1 word ~ 4.7 characters. So, 1 chunk <= 1000 characters, otherwise it is\n",
        "# truncated. Alltogether, we end up with at least 3 chunks per page at a\n",
        "# chunk_size of 1000.]\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                                  chunk_size=1000,\n",
        "                                  chunk_overlap=100,\n",
        "                                  separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"])\n",
        "\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# Choose the Embedding Model.\n",
        "# [I tried to find the best embedding model via the MTEB leaderboard at\n",
        "# huggingface.co:\n",
        "# 1) nvidia/NV-Embed-v2 (not found on NVIDIA website),\n",
        "# 2) BAAI/bge-en-icl (runs forever)],\n",
        "# ...\n",
        "# 10) nvidia/NV-Embed-v1 (needed packages incompatible).\n",
        "# Hence, I ended up with the following standard model.]\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Add the Chunks to the Vector Database.\n",
        "vectorstore = FAISS.from_texts(texts=chunks, embedding=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji0rJ2iJNXg5"
      },
      "source": [
        "## Main components of RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DMr8jMWtNI6A"
      },
      "outputs": [],
      "source": [
        "# Retriever.\n",
        "# [Similarity search with a threshold: search_type=\"similarity_score_threshold\",\n",
        "# search_kwargs={\"score_threshold\": 0.05}]\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Systemprompt.\n",
        "template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context:  {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Set LLM.\n",
        "llm = ChatMistralAI(mistral_api_key=YOUR_API_KEY")\n",
        "\n",
        "# Create Pipeline (Retrieve-Augment-Generate)\n",
        "RAG_chain = ({\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "              | prompt\n",
        "              | llm\n",
        "              | StrOutputParser())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzs6zs16Nfas"
      },
      "source": [
        "## Q&A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zoai5pfMRkco",
        "outputId": "8a2ab345-f54e-4c54-afe8-4443ce2aebb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The context mentions that people \"weren\\'t invited\" to Gatsby\\'s parties, they just went there. However, specific individuals named Nick, Jordan, and Lucille were invited. The host, Gatsby, is also reported to have asked for a girl\\'s name and address, and later sent her a package containing a new evening gown.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Generate.\n",
        "query = \"Who was invited to the parties?\"\n",
        "RAG_chain.invoke(query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxWf/XEeK2TCqj0eM9sizq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
