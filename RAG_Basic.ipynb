{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elektromusik/RAG/blob/main/RAG_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwrNP404M7r5"
      },
      "source": [
        "# RAG with Langchain and Mistral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgqxGDjYNBWv"
      },
      "source": [
        "## Load Packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WXApMi-sJd9v"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet faiss-cpu langchain langchain_community langchain_mistralai\n",
        "!pip install --quiet sentence_transformers\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_mistralai.chat_models import ChatMistralAI\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWz4RVK5NE5c"
      },
      "source": [
        "## Data Preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GWp5u52I5_2",
        "outputId": "ddd30b8d-8377-4253-9760-af96c50ff477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Import the Text.\n",
        "with open(\"The_Great_Gatsby.txt\") as x:\n",
        "  text = x.read()\n",
        "\n",
        "# Chunk the Text\n",
        "# [1 page ~ 700 words. 1 chunk <= 256 words (due to the embedding model).\n",
        "# 1 word ~ 4.7 characters. So, 1 chunk <= 1000 characters, otherwise it is\n",
        "# truncated. Alltogether, we end up with at least 3 chunks per page at a\n",
        "# chunk_size of 1000.]\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "                                  chunk_size=1000,\n",
        "                                  chunk_overlap=100,\n",
        "                                  separators=[\"\\n\\n\", \"\\n\", \".\", \",\", \" \", \"\"])\n",
        "\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# Choose the Embedding Model.\n",
        "# [I tried to find the best embedding model via the MTEB leaderboard at\n",
        "# huggingface.co:\n",
        "# 1) nvidia/NV-Embed-v2 (not found on NVIDIA website),\n",
        "# 2) BAAI/bge-en-icl (runs forever)],\n",
        "# ...\n",
        "# 10) nvidia/NV-Embed-v1 (needed packages incompatible).\n",
        "# Hence, I ended up with the following standard model.]\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Add the Chunks to the Vector Database.\n",
        "vectorstore = FAISS.from_texts(texts=chunks, embedding=embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji0rJ2iJNXg5"
      },
      "source": [
        "## Main components of RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DMr8jMWtNI6A"
      },
      "outputs": [],
      "source": [
        "# Retriever.\n",
        "# [Similarity search with a threshold: search_type=\"similarity_score_threshold\",\n",
        "# search_kwargs={\"score_threshold\": 0.05}]\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Systemprompt.\n",
        "template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "If you don't know the answer, just say that you don't know.\n",
        "Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context:  {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Set LLM.\n",
        "llm = ChatMistralAI(mistral_api_key=\"QlvclnycvhnkP808e4HS0BWz0kwZU06j\")\n",
        "\n",
        "# Create Pipeline (Retrieve-Augment-Generate)\n",
        "RAG_chain = ({\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
        "              | prompt\n",
        "              | llm\n",
        "              | StrOutputParser())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dzs6zs16Nfas"
      },
      "source": [
        "## Q&A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "zoai5pfMRkco",
        "outputId": "ab694243-b1c4-4a2d-baa2-aa7a957aa4af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The documents mention various foods at Gatsby's parties, including oranges and lemons, spiced baked hams, salads, pastry pigs and turkeys, and hors-d'oeuvre. There's also a bar with gins, liquors, and cordials. The first supper served at a party had married couples and Jordan's escort. The specific food consumed at the dinner is not detailed.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Generate.\n",
        "query = \"What foods were served at the parties?\"\n",
        "RAG_chain.invoke(query)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpRwoGw+zPAog5zpdcfpEb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}